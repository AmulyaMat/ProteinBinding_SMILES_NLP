{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"},{"sourceId":8875778,"sourceType":"datasetVersion","datasetId":5342782}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objective: Toeknization of SMILES data to predict Binding Affinity with CHEMBert Model","metadata":{}},{"cell_type":"markdown","source":"Some papers: \nSMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction\nhttps://dl.acm.org/doi/abs/10.1145/3307339.3342186?casa_token=x0RGiJZbcM8AAAAA:Jpdz0mKm8oQAjhXj4Z0BDEbcR4WoZ8v3113op2iUXx1I4erUs4t6i7ubPdBPNe5A4pN0LUiPuFveog\n\nPySMILESUtils â€“ Enabling deep learning with the SMILES chemical language:\nchrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/60d99ad2fca490cddfc97083/original/py-smiles-utils-enabling-deep-learning-with-the-smiles-chemical-language.pdf\n","metadata":{}},{"cell_type":"markdown","source":"![](https://ars.els-cdn.com/content/image/1-s2.0-S2001037021000763-gr4.jpg)","metadata":{}},{"cell_type":"markdown","source":"https://www.sciencedirect.com/science/article/pii/S2001037021000763","metadata":{}},{"cell_type":"markdown","source":"## 1. Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install rdkit\n!pip install duckdb\n!pip install transformers pandas torch","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:51:19.696504Z","iopub.execute_input":"2024-09-07T00:51:19.696945Z","iopub.status.idle":"2024-09-07T00:51:58.096182Z","shell.execute_reply.started":"2024-09-07T00:51:19.696901Z","shell.execute_reply":"2024-09-07T00:51:58.094803Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rdkit in /opt/conda/lib/python3.10/site-packages (2024.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit) (9.5.0)\nRequirement already satisfied: duckdb in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.0.dev0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:51:58.098535Z","iopub.execute_input":"2024-09-07T00:51:58.098888Z","iopub.status.idle":"2024-09-07T00:51:59.079275Z","shell.execute_reply.started":"2024-09-07T00:51:58.098856Z","shell.execute_reply":"2024-09-07T00:51:59.077917Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The training set is pretty big, but we can treat the parquet files as databases using duckdb. We will use this to sample down to a smaller dataset for demonstration purposes. Lets install duckdb as well.","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Preparation (Train & Test)\n\nThe training and testing data paths are defined for the .parquet files. We use duckdb to scan search through the large training sets. Just to get started lets sample out an equal number of positive and negatives. \n\nThis query selects an equal number of samples where binds equals 0 (non-binding) and 1 (binding), limited to 30,000 each, to avoid model bias towards a particular class.","metadata":{}},{"cell_type":"markdown","source":"--> changed to 100","metadata":{}},{"cell_type":"code","source":"import duckdb\nimport pandas as pd\n\ntrain_path = '/kaggle/input/leash-BELKA/train.parquet'\ntest_path = '/kaggle/input/leash-BELKA/test.parquet'\n\ncon = duckdb.connect()\n\ndf = con.query(f\"\"\"(SELECT *\n                        FROM parquet_scan('{train_path}')\n                        WHERE binds = 0\n                        ORDER BY random()\n                        LIMIT 30000)\n                        UNION ALL\n                        (SELECT *\n                        FROM parquet_scan('{train_path}')\n                        WHERE binds = 1\n                        ORDER BY random()\n                        LIMIT 30000)\"\"\").df()\n\ncon.close()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:51:59.080856Z","iopub.execute_input":"2024-09-07T00:51:59.081500Z","iopub.status.idle":"2024-09-07T00:52:52.165729Z","shell.execute_reply.started":"2024-09-07T00:51:59.081454Z","shell.execute_reply":"2024-09-07T00:52:52.164373Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"188b19c042d2436cb9c09251bb8fa744"}},"metadata":{}}]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:52:52.207736Z","iopub.execute_input":"2024-09-07T00:52:52.208258Z","iopub.status.idle":"2024-09-07T00:52:52.228206Z","shell.execute_reply.started":"2024-09-07T00:52:52.208223Z","shell.execute_reply":"2024-09-07T00:52:52.226851Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"          id                              buildingblock1_smiles  \\\n0  242610268  O=C(O)COc1cccc(-c2csc(NC(=O)OCC3c4ccccc4-c4ccc...   \n1  239782150                O=C(O)CNC(=O)OCC1c2ccccc2-c2ccccc21   \n2   11752955  CC(=O)c1ccc(C[C@H](NC(=O)OCC2c3ccccc3-c3ccccc3...   \n3   46163777  Cc1c(Br)ccc(C(=O)O)c1NC(=O)OCC1c2ccccc2-c2ccccc21   \n4   50566520      Cc1cc(C(=O)O)ccc1NC(=O)OCC1c2ccccc2-c2ccccc21   \n5   40479064     COc1nccc(C(=O)O)c1NC(=O)OCC1c2ccccc2-c2ccccc21   \n6  214520933      O=C(Nc1cccc(I)c1C(=O)O)OCC1c2ccccc2-c2ccccc21   \n7  261042013  O=C(O)C[C@@H](NC(=O)OCC1c2ccccc2-c2ccccc21)c1c...   \n8   32915901  COc1cc(NC(=O)OCC2c3ccccc3-c3ccccc32)c(C(=O)O)c...   \n9   38421201     COc1cccc(C(=O)O)c1NC(=O)OCC1c2ccccc2-c2ccccc21   \n\n      buildingblock2_smiles        buildingblock3_smiles  \\\n0         NCC1CCC(C(F)F)CC1             Cl.NC1CCC(=O)CC1   \n1  CCN1CCN(Cc2ccc(N)nc2)CC1  Nc1cc(F)cc(F)c1[N+](=O)[O-]   \n2           Nc1cc(Cl)cnc1Cl     Cc1nc(N)ccc1[N+](=O)[O-]   \n3      Cc1ccccc1-c1csc(N)n1              Cc1cc(CN)c(C)o1   \n4      Cc1sc(C)c(CN)c1Br.Cl           Nc1nc(Br)cn2ccnc12   \n5       CNC(=O)c1ccc(N)cc1F                 Nc1cncc(F)c1   \n6      Nc1cccc(CN2CCCCC2)c1                     Nc1nncs1   \n7       Cl.NCc1csc(=O)[nH]1     NCCc1ccc(N2CCOCC2)c(F)c1   \n8     COc1c(F)ccc(F)c1CN.Cl                  Cc1cnc(N)s1   \n9             COc1cnc(N)cn1        Cl.NCCn1cnc2sccc2c1=O   \n\n                                     molecule_smiles protein_name  binds  \n0  O=C1CCC(Nc2nc(NCC3CCC(C(F)F)CC3)nc(Nc3nc(-c4cc...          HSA      0  \n1  CCN1CCN(Cc2ccc(Nc3nc(NCC(=O)N[Dy])nc(Nc4cc(F)c...          HSA      0  \n2  CC(=O)c1ccc(C[C@H](Nc2nc(Nc3ccc([N+](=O)[O-])c...          sEH      0  \n3  Cc1cc(CNc2nc(Nc3nc(-c4ccccc4C)cs3)nc(Nc3c(C(=O...          sEH      0  \n4  Cc1cc(C(=O)N[Dy])ccc1Nc1nc(NCc2c(C)sc(C)c2Br)n...          sEH      0  \n5  CNC(=O)c1ccc(Nc2nc(Nc3cncc(F)c3)nc(Nc3c(C(=O)N...          HSA      0  \n6  O=C(N[Dy])c1c(I)cccc1Nc1nc(Nc2cccc(CN3CCCCC3)c...          sEH      0  \n7  O=C(C[C@@H](Nc1nc(NCCc2ccc(N3CCOCC3)c(F)c2)nc(...          HSA      0  \n8  COc1cc(Nc2nc(NCc3c(F)ccc(F)c3OC)nc(Nc3ncc(C)s3...         BRD4      0  \n9  COc1cnc(Nc2nc(NCCn3cnc4sccc4c3=O)nc(Nc3c(OC)cc...         BRD4      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>buildingblock1_smiles</th>\n      <th>buildingblock2_smiles</th>\n      <th>buildingblock3_smiles</th>\n      <th>molecule_smiles</th>\n      <th>protein_name</th>\n      <th>binds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>242610268</td>\n      <td>O=C(O)COc1cccc(-c2csc(NC(=O)OCC3c4ccccc4-c4ccc...</td>\n      <td>NCC1CCC(C(F)F)CC1</td>\n      <td>Cl.NC1CCC(=O)CC1</td>\n      <td>O=C1CCC(Nc2nc(NCC3CCC(C(F)F)CC3)nc(Nc3nc(-c4cc...</td>\n      <td>HSA</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239782150</td>\n      <td>O=C(O)CNC(=O)OCC1c2ccccc2-c2ccccc21</td>\n      <td>CCN1CCN(Cc2ccc(N)nc2)CC1</td>\n      <td>Nc1cc(F)cc(F)c1[N+](=O)[O-]</td>\n      <td>CCN1CCN(Cc2ccc(Nc3nc(NCC(=O)N[Dy])nc(Nc4cc(F)c...</td>\n      <td>HSA</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11752955</td>\n      <td>CC(=O)c1ccc(C[C@H](NC(=O)OCC2c3ccccc3-c3ccccc3...</td>\n      <td>Nc1cc(Cl)cnc1Cl</td>\n      <td>Cc1nc(N)ccc1[N+](=O)[O-]</td>\n      <td>CC(=O)c1ccc(C[C@H](Nc2nc(Nc3ccc([N+](=O)[O-])c...</td>\n      <td>sEH</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>46163777</td>\n      <td>Cc1c(Br)ccc(C(=O)O)c1NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n      <td>Cc1ccccc1-c1csc(N)n1</td>\n      <td>Cc1cc(CN)c(C)o1</td>\n      <td>Cc1cc(CNc2nc(Nc3nc(-c4ccccc4C)cs3)nc(Nc3c(C(=O...</td>\n      <td>sEH</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50566520</td>\n      <td>Cc1cc(C(=O)O)ccc1NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n      <td>Cc1sc(C)c(CN)c1Br.Cl</td>\n      <td>Nc1nc(Br)cn2ccnc12</td>\n      <td>Cc1cc(C(=O)N[Dy])ccc1Nc1nc(NCc2c(C)sc(C)c2Br)n...</td>\n      <td>sEH</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>40479064</td>\n      <td>COc1nccc(C(=O)O)c1NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n      <td>CNC(=O)c1ccc(N)cc1F</td>\n      <td>Nc1cncc(F)c1</td>\n      <td>CNC(=O)c1ccc(Nc2nc(Nc3cncc(F)c3)nc(Nc3c(C(=O)N...</td>\n      <td>HSA</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>214520933</td>\n      <td>O=C(Nc1cccc(I)c1C(=O)O)OCC1c2ccccc2-c2ccccc21</td>\n      <td>Nc1cccc(CN2CCCCC2)c1</td>\n      <td>Nc1nncs1</td>\n      <td>O=C(N[Dy])c1c(I)cccc1Nc1nc(Nc2cccc(CN3CCCCC3)c...</td>\n      <td>sEH</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>261042013</td>\n      <td>O=C(O)C[C@@H](NC(=O)OCC1c2ccccc2-c2ccccc21)c1c...</td>\n      <td>Cl.NCc1csc(=O)[nH]1</td>\n      <td>NCCc1ccc(N2CCOCC2)c(F)c1</td>\n      <td>O=C(C[C@@H](Nc1nc(NCCc2ccc(N3CCOCC3)c(F)c2)nc(...</td>\n      <td>HSA</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>32915901</td>\n      <td>COc1cc(NC(=O)OCC2c3ccccc3-c3ccccc32)c(C(=O)O)c...</td>\n      <td>COc1c(F)ccc(F)c1CN.Cl</td>\n      <td>Cc1cnc(N)s1</td>\n      <td>COc1cc(Nc2nc(NCc3c(F)ccc(F)c3OC)nc(Nc3ncc(C)s3...</td>\n      <td>BRD4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>38421201</td>\n      <td>COc1cccc(C(=O)O)c1NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n      <td>COc1cnc(N)cn1</td>\n      <td>Cl.NCCn1cnc2sccc2c1=O</td>\n      <td>COc1cnc(Nc2nc(NCCn3cnc4sccc4c3=O)nc(Nc3c(OC)cc...</td>\n      <td>BRD4</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:52:57.355526Z","iopub.execute_input":"2024-09-07T00:52:57.355993Z","iopub.status.idle":"2024-09-07T00:52:57.364534Z","shell.execute_reply.started":"2024-09-07T00:52:57.355956Z","shell.execute_reply":"2024-09-07T00:52:57.362983Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(60000, 7)"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3. Tokenization using SmilesPE & creating DataLoader\n","metadata":{}},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q SmilesPE\n!pip install --upgrade deepsmiles","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:53:22.387237Z","iopub.execute_input":"2024-09-07T00:53:22.388118Z","iopub.status.idle":"2024-09-07T00:54:11.871630Z","shell.execute_reply.started":"2024-09-07T00:53:22.388071Z","shell.execute_reply":"2024-09-07T00:54:11.869616Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Example usage of SmilesPE","metadata":{}},{"cell_type":"code","source":"from SmilesPE.pretokenizer import atomwise_tokenizer\n\nsmi = 'CC[N+](C)(C)Cc1ccccc1Br'\ntoks = atomwise_tokenizer(smi)\nprint(toks)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T00:54:29.930204Z","iopub.execute_input":"2024-09-07T00:54:29.930612Z","iopub.status.idle":"2024-09-07T00:54:29.938078Z","shell.execute_reply.started":"2024-09-07T00:54:29.930582Z","shell.execute_reply":"2024-09-07T00:54:29.936551Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['C', 'C', '[N+]', '(', 'C', ')', '(', 'C', ')', 'C', 'c', '1', 'c', 'c', 'c', 'c', 'c', '1', 'Br']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Functions","metadata":{}},{"cell_type":"code","source":"def tokenize_smiles(smiles, tokenizer):\n    if pd.isna(smiles):\n        return []\n    return tokenizer.tokenize(smiles)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Padding and attention mask generation\ndef pad_sequences(sequences, max_length=None, padding_value=0):\n    if not max_length:\n        max_length = max(len(seq) for seq in sequences)\n    \n    padded_sequences = []\n    attention_masks = []\n    \n    for seq in sequences:\n        # Padding the sequence\n        padded_seq = seq + [padding_value] * (max_length - len(seq))\n        padded_sequences.append(padded_seq)\n        \n        # Attention mask: 1 for real tokens, 0 for padding\n        attention_mask = [1] * len(seq) + [0] * (max_length - len(seq))\n        attention_masks.append(attention_mask)\n    \n    return padded_sequences, attention_masks\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:09:38.142085Z","iopub.execute_input":"2024-09-07T02:09:38.142550Z","iopub.status.idle":"2024-09-07T02:09:38.151130Z","shell.execute_reply.started":"2024-09-07T02:09:38.142518Z","shell.execute_reply":"2024-09-07T02:09:38.149502Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Apply padding and generate attention masks for the train and validation sets\ntrain_texts_padded, train_attention_mask = pad_sequences(train_texts)\nval_texts_padded, val_attention_mask = pad_sequences(val_texts)\n\n# Convert padded sequences and attention masks into tensors\ntrain_texts_tensor = torch.tensor(train_texts_padded)\nval_texts_tensor = torch.tensor(val_texts_padded)\ntrain_attention_mask_tensor = torch.tensor(train_attention_mask)\nval_attention_mask_tensor = torch.tensor(val_attention_mask)\n\n# Convert train_labels and val_labels to tensors (as before)\ntrain_labels_tensor = torch.tensor(train_labels, dtype=torch.long)  # For classification\nval_labels_tensor = torch.tensor(val_labels, dtype=torch.long)      # For classification\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:09:48.486296Z","iopub.execute_input":"2024-09-07T02:09:48.486760Z","iopub.status.idle":"2024-09-07T02:09:51.619985Z","shell.execute_reply.started":"2024-09-07T02:09:48.486729Z","shell.execute_reply":"2024-09-07T02:09:51.618823Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"**Dataloader generation**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Function to convert tokenized SMILES into sequences of indices\ndef tokens_to_indices(tokens, vocab):\n    return [vocab[token] for token in tokens if token in vocab]\n\n# Convert tokenized SMILES into sequences of indices\ndf['indexed_smiles'] = df['tokenized_smiles'].apply(lambda x: tokens_to_indices(x, vocab))\n\n# Split data into train and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(df['indexed_smiles'], df['binds'], test_size=0.2)\n\n# Padding and truncation: Since the tokens are sequences of varying length, we'll pad them to the same length\ndef pad_sequences(sequences, max_length=None, padding_value=0):\n    if not max_length:\n        max_length = max(len(seq) for seq in sequences)\n    padded_sequences = [seq + [padding_value] * (max_length - len(seq)) for seq in sequences]\n    return padded_sequences\n\n# Apply padding to the train and validation texts\ntrain_texts_padded = pad_sequences(train_texts)\nval_texts_padded = pad_sequences(val_texts)\n\n# Convert the padded sequences into tensors\ntrain_texts_tensor = torch.tensor(train_texts_padded)\nval_texts_tensor = torch.tensor(val_texts_padded)\n\n# Convert train_labels and val_labels from pandas Series to lists (if they are pandas Series)\ntrain_labels = train_labels.tolist()\nval_labels = val_labels.tolist()\n\n# Convert labels to tensors\ntrain_labels_tensor = torch.tensor(train_labels, dtype=torch.long)  # Use torch.long for classification labels\nval_labels_tensor = torch.tensor(val_labels, dtype=torch.long)      # Use torch.long for classification labels\n\n\nclass SmilesDataset(Dataset):\n    def __init__(self, encodings, attention_masks, labels):\n        self.encodings = encodings\n        self.attention_masks = attention_masks\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings[idx].clone().detach(),          # Clone and detach the tensor\n            'attention_mask': self.attention_masks[idx].clone().detach(),  # Clone and detach the tensor\n            'labels': self.labels[idx].clone().detach()                 # Clone and detach the tensor\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:10:17.063029Z","iopub.execute_input":"2024-09-07T02:10:17.063474Z","iopub.status.idle":"2024-09-07T02:10:19.598465Z","shell.execute_reply.started":"2024-09-07T02:10:17.063439Z","shell.execute_reply":"2024-09-07T02:10:19.597260Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = SmilesDataset(train_texts_tensor, train_attention_mask_tensor, train_labels_tensor)\nval_dataset = SmilesDataset(val_texts_tensor, val_attention_mask_tensor, val_labels_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:10:36.447687Z","iopub.execute_input":"2024-09-07T02:10:36.448251Z","iopub.status.idle":"2024-09-07T02:10:36.454438Z","shell.execute_reply.started":"2024-09-07T02:10:36.448211Z","shell.execute_reply":"2024-09-07T02:10:36.452923Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:10:44.686751Z","iopub.execute_input":"2024-09-07T02:10:44.687212Z","iopub.status.idle":"2024-09-07T02:10:44.697521Z","shell.execute_reply.started":"2024-09-07T02:10:44.687173Z","shell.execute_reply":"2024-09-07T02:10:44.695810Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Inspect the first item in the val_dataset\nfirst_item = val_dataset[0]\nprint(\"First Item Input IDs (Tokenized SMILES):\", first_item['input_ids'])\nprint(\"First Item Label (Binds):\", first_item['labels'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:10:49.961323Z","iopub.execute_input":"2024-09-07T02:10:49.961775Z","iopub.status.idle":"2024-09-07T02:10:49.971853Z","shell.execute_reply.started":"2024-09-07T02:10:49.961741Z","shell.execute_reply":"2024-09-07T02:10:49.970347Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"First Item Input IDs (Tokenized SMILES): tensor([ 6, 26,  3,  7,  4,  7,  7,  5,  6,  7,  8,  9,  7,  5,  6,  7, 10,  7,\n         7,  5,  6, 14,  3,  3,  6,  3,  3, 14, 12,  7,  7,  7, 10, 17,  5,  2,\n         1, 12, 18, 12,  9,  7,  5,  6, 24,  5,  3,  3,  5,  2,  1, 12,  6, 15,\n        12,  3,  7, 10,  7,  7,  7,  5, 17,  5,  2,  1, 12, 18, 12,  7,  7, 10,\n        12,  9,  8, 12,  7,  7,  7,  4, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\nFirst Item Label (Binds): tensor(1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Model Training with CHEMBert Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# Load the ChemBERTa tokenizer and model for classification\ntokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\", num_labels=2)  # Assuming binary classification","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:07:44.280831Z","iopub.execute_input":"2024-09-07T02:07:44.281344Z","iopub.status.idle":"2024-09-07T02:07:44.702845Z","shell.execute_reply.started":"2024-09-07T02:07:44.281305Z","shell.execute_reply":"2024-09-07T02:07:44.701486Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1614: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Length of train_texts_padded: {len(train_texts_padded)}\")\nprint(f\"Length of train_labels: {len(train_labels)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:11:04.694029Z","iopub.execute_input":"2024-09-07T02:11:04.694476Z","iopub.status.idle":"2024-09-07T02:11:04.700740Z","shell.execute_reply.started":"2024-09-07T02:11:04.694443Z","shell.execute_reply":"2024-09-07T02:11:04.699404Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Length of train_texts_padded: 48000\nLength of train_labels: 48000\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(train_texts_padded))\nprint(type(train_labels))","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:11:06.492896Z","iopub.execute_input":"2024-09-07T02:11:06.493321Z","iopub.status.idle":"2024-09-07T02:11:06.499538Z","shell.execute_reply.started":"2024-09-07T02:11:06.493289Z","shell.execute_reply":"2024-09-07T02:11:06.498247Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"<class 'list'>\n<class 'list'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AdamW\nfrom torch import nn\n\n# Define optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=5e-5)\nloss_fn = nn.CrossEntropyLoss()\n\n# Move model to GPU if available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Training loop with accuracy calculation\nfor epoch in range(3):  # Train for 3 epochs\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    total_samples = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)  # Include attention mask\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        # Extract loss and logits from the model outputs\n        loss = outputs.loss\n        logits = outputs.logits  # Get the logits (raw predictions)\n    \n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Calculate accuracy\n        _, predicted_labels = torch.max(logits, dim=1)  # Get the index of the max logit (predicted class)\n        correct_predictions += (predicted_labels == labels).sum().item()\n        total_samples += labels.size(0)  # Count the number of samples in the batch\n\n    # Calculate average loss and accuracy for the epoch\n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct_predictions / total_samples\n\n    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Training Accuracy = {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T02:13:42.573406Z","iopub.execute_input":"2024-09-07T02:13:42.573927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['buildingblock1_tokens'] = df['buildingblock1_smiles'].apply(lambda x: tokenize_smiles(x, spe))\ndf['buildingblock2_tokens'] = df['buildingblock2_smiles'].apply(lambda x: tokenize_smiles(x, spe))\ndf['buildingblock3_tokens'] = df['buildingblock3_smiles'].apply(lambda x: tokenize_smiles(x, spe))\ndf['molecule_tokens'] = df['molecule_smiles'].apply(lambda x: tokenize_smiles(x, spe))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}